{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/Fundamentals/NN_Fundamentals.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/NN_Fundamentals.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[:,0], X[:,1], c=y.ravel(), cmap=plt.cm.Spectral)\n",
    "    \n",
    "def load_planar_dataset(m, sigma=0.2):\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*sigma # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*sigma # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    ## premuting ouput\n",
    "    p = np.random.permutation(m)\n",
    "    X = X[p,:]\n",
    "    Y = Y[p,:]\n",
    "\n",
    "    return X, Y.ravel()\n",
    "\n",
    "def draw_neural_net(left, right, bottom, top, layer_sizes):\n",
    "    '''\n",
    "    From https://gist.github.com/craffel/2d727968c3aaebd10359#file-draw_neural_net-py\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    ax = fig.gca()\n",
    "    ax.axis('off')\n",
    "\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size):\n",
    "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "                                color='w', ec='k', zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "    # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                  [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                ax.add_artist(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "We have a binary classification dataset. The target `y` has two possible values, {0,1} and the input vector `x` has two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "X, y = load_planar_dataset(N, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dataset using matplotlib. The data looks like a \"flower\" with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[:,0], X[:,1], c=y.ravel(), s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape:{0}, y shape:{1}'.format(X.shape, y.shape))\n",
    "print('X_train shape:{0}, y_train shape:{1}'.format(X_train.shape,\n",
    "                                                    y_train.shape))\n",
    "print('X_test shape:{0}, y_test shape:{1}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X first example\n",
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y firsts examples\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First NN in Keras\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "Fully connected layers are defined using the [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) class. We can specify the number of neurons as the first argument, and specify the activation function using the activation argument.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dense(\n",
    "    units, activation=None\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "- **units**: Number of neurons in the layer.\n",
    "- **activation**: Activation function to use, default function is `linear`, you can also use `sigmoid` for sigmoid activation or [other alternatives](https://www.tensorflow.org/api_docs/python/tf/keras/activations) \n",
    "\n",
    "For defining the input layer you can use `keras.Input(shape=(input_dimension,))` or define your input dimension in the first hidden layer.\n",
    "\n",
    "```python\n",
    "layers.Dense(4, activation=\"sigmoid\", input_shape=(input_dimension,))\n",
    "\n",
    "```\n",
    "\n",
    "Build a network without hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net(.1, .9, .1, .9, [2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# The dimension of the input is 2 (planar data)\n",
    "#Â we have only one output layer with 1 neuron\n",
    "model.add(keras.layers.Dense(1, input_shape=(2, ), activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile\n",
    "Now we need to specify the loss function, the metrics and the optimizer. It is done using compile function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model\n",
    "This is similar to a sklearn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10,\n",
    "          validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred[:3], y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)\n",
    "plt.title(\"Basic NN\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â One hidden layer NN\n",
    "\n",
    "- One **input with 2 features (input_shape)**\n",
    "- **One hidden dense layer**,  with **3 neurons** and implemented with  **sigmoid activating function**\n",
    "- **One Output** (sigmoid output activation function)\n",
    "\n",
    "We can create layers incrementally with the method `add()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net(.1, .9, .1, .9, [2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "## hidden layer with 3 neurons and sigmoid activation\n",
    "## The first layer need to have the input_shape\n",
    "model.add(layers.Dense(3, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "#Â output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define the model with a list of layers\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(2, )),\n",
    "    layers.Dense(3, activation='sigmoid', name='layer1'),\n",
    "    layers.Dense(1, activation='sigmoid', name='layer3')\n",
    "])\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=25, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)\n",
    "plt.title(\"1 Hidden layer NN\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "####  What happens if we change the activation function of the first hidden  layer to `'linear'`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "## The first layer need to have the input_shape\n",
    "## change the activation function\n",
    "model.add(layers.Dense(3, input_shape=(2,), activation=...))\n",
    "#Â output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=25, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))\n",
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)\n",
    "plt.title(\"1 Hidden layer NN, linear activation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "####  What happens if we add more neurons to the first hidden layer ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net(.1, .9, .1, .9, [2, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "## The first layer need to have the input_shape\n",
    "## set the number of neurons\n",
    "model.add(layers.Dense(..., input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "#Â output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=25, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))\n",
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)\n",
    "plt.title(\"1 Hidden layer NN, linear activation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "####  What happens if we add one more hidden layer ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net(.1, .9, .1, .9, [2, 3, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# First hidden layer (need to innclude input_shape)\n",
    "## Set the number of neurons\n",
    "model.add(layers.Dense(..., input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "# second hidden layer\n",
    "##Â Complete the code\n",
    "model.add(...\n",
    "\n",
    "#Â output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=25, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))\n",
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)\n",
    "plt.title(\"1 Hidden layer NN, linear activation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Use  the [hyperbolic tangent](https://mathworld.wolfram.com/HyperbolicTangent.html) activation `'tanh'` with the first simple model and compare the results with the sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "## The first layer need to have the input_shape\n",
    "## change the activation function\n",
    "model.add(layers.Dense(3, input_shape=(2,), activation=...))\n",
    "#Â output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=25, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))\n",
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)\n",
    "plt.title(\"1 Hidden layer NN, linear activation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Practice I\n",
    "\n",
    "Repeat the process with a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "X, y = sklearn.datasets.make_moons(n_samples=N, noise=.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[:,0], X[:,1], c=y.ravel(), s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    ")\n",
    "print('X shape:{0}, y shape:{1}'.format(X.shape, y.shape))\n",
    "print('X_train shape:{0}, y_train shape:{1}'.format(X_train.shape,\n",
    "                                                    y_train.shape))\n",
    "print('X_test shape:{0}, y_test shape:{1}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "## The first layer need to have the input_shape\n",
    "model.add(layers...(..., input_shape=(...,), activation=...))\n",
    "\n",
    "## Second Layer, don't use input_shape\n",
    "model...(...)\n",
    "\n",
    "#Â output layer\n",
    "model.add(layers.Dense(..., activation=...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=35, validation_split=0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))\n",
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Functional API\n",
    "\n",
    "The Keras Functional API allows defining the topology of the models as a [acyclic directed graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of layers. Next we will build the same model as in the previous section but using the Keras Functional API.\n",
    "\n",
    "You need to start with an input data entry:\n",
    "```python    \n",
    "    inputs = keras.Input(shape=(8,))\n",
    "```\n",
    "    \n",
    "We define the layers that we will include in the network and the relationships between them. It is important to note that each line details the type of layer and its properties (number of neurons, activation function, name ...) and just at the end, in parentheses, the name of the variable in which we have saved the layer from which its output will connect to the input of the layer we are declaring.\n",
    "```python\n",
    "layer_1 = layers.Dense(4, activation='sigmoid',\n",
    "                       name='layer_1')(inputs)\n",
    "layer_2 = layers.Dense(4, activation='sigmoid',\n",
    "                       name='layer_2')(layer_1)\n",
    "```\n",
    "\n",
    "The \"layer call\" `Dense()(inputs)` action is like drawing an arrow from \"inputs\" to this layer you created. You're \"passing\" the inputs to the dense layer, and you get `layer_1` as the output.\n",
    "\n",
    "  \n",
    "\n",
    "Finally, we define the network outputs:\n",
    "```python\n",
    "outputs = layers.Dense(1, name='output_layer')(layer_2)\n",
    "```\n",
    "\n",
    "We create the model specifying the inputs and outputs:\n",
    "```python\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net(.1, .9, .1, .9, [8, 4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(8, ), name='input_layer')\n",
    "l_1 = layers.Dense(4, activation='relu', name='layer_1')(inputs)\n",
    "l_2 = layers.Dense(4, activation='relu', name='layer_2')(l_1)\n",
    "outputs = layers.Dense(1, name='output_layer')(l_2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='example_model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Practice II\n",
    "\n",
    "Repeat the process with a different dataset and the **Functional API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "X, y = sklearn.datasets.make_circles(n_samples=N, factor=.65, noise=.065) # sklearn.datasets.make_moons(n_samples=N, noise=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[:,0], X[:,1], c=y.ravel(), s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    ")\n",
    "print('X shape:{0}, y shape:{1}'.format(X.shape, y.shape))\n",
    "print('X_train shape:{0}, y_train shape:{1}'.format(X_train.shape,\n",
    "                                                    y_train.shape))\n",
    "print('X_test shape:{0}, y_test shape:{1}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(2, ), name='input_layer')\n",
    "## First hidden layer\n",
    "l_1 = layers.Dense(...)(inputs)\n",
    "...\n",
    "\n",
    "## output layer\n",
    "outputs = layers.Dense(...\n",
    "                       \n",
    "\n",
    "model = keras.Model(inputs=inputs,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â compiling and training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=35, validation_split=0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))\n",
    "plot_decision_boundary(lambda x: (\n",
    "    model.predict(x) > 0.5).astype(\"int32\"), X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
